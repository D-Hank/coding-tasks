import math

from concurrent.futures import ProcessPoolExecutor, as_completed
from human_eval.data import read_problems, stream_jsonl

from execute import run_code

import_lib = ""

# Compute pass@k score of a certain problem
def compute_score(n: int, c: int, k: int) -> float:
    # Simple cases
    if k > n or c == 0:
        return 0.0
    
    if n - c < k:
        return 1.0

    return 1 - math.comb(n - c, k) / math.comb(n, k)

def evaluate(k = 1, num_workers = 8) -> float:
    # Read dataset input
    problems = read_problems("../../datasets/humaneval/HumanEval.jsonl")
    # Read samples generated by LLM
    samples = stream_jsonl("samples.jsonl")
    task_ids = problems.keys()
    # n and c of each task
    task_n_c = {task_id : {"n" : 0, "c" : 0} for task_id in task_ids}

    # Put all tasks in queue, assign our worker processes to them
    with ProcessPoolExecutor(num_workers) as pool:
        procs = []
        for sample in samples:
            task_id = sample["task_id"]
            completion = sample["completion"]

            # Fetch input
            problem = problems[task_id]
            prompt = problem["prompt"]
            test = problem["test"]
            candidate = problem["entry_point"]
            code = import_lib + "\n" + prompt + "\n" + completion + "\n" + test + "\ncheck(" + candidate + ")\n"

            # Submit to process
            proc = pool.submit(run_code, task_id, code)
            procs.append(proc)

        # Check each process's status
        for stat in as_completed(procs):
            # No need to save info to compute score
            task_id, ret = stat.result()
            task_n_c[task_id]["n"] += 1
            task_n_c[task_id]["c"] += int(ret[0])

    # Compute pass@k of each problem
    scores = []
    for nc in task_n_c.values():
        n, c = nc["n"], nc["c"]
        score = compute_score(n, c, k)
        scores.append(score)

    return sum(scores) / len(scores)

if __name__ == "__main__":
    pass_k = evaluate()
    print("pass@1:", pass_k)
